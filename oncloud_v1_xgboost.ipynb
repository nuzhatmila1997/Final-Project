{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and evaluate ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, boto3, zipfile, requests\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='c182567a4701745l12374482t1w262869721852-labbucket-jw0boyepjqpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 228982  Validate: 49068  Test: 49068\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('combined_csv_v1_20percent.csv')\n",
    "\n",
    "# label column is named 'target' and is first column \n",
    "if 'target' not in df.columns:\n",
    "    raise ValueError(\"Expected a 'target' column in the dataset.\")\n",
    "cols = df.columns.tolist()\n",
    "cols = ['target'] + [c for c in cols if c != 'target']\n",
    "df = df[cols]\n",
    "\n",
    "# Optional: ensure target is 0/1 ints\n",
    "df['target'] = pd.to_numeric(df['target'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Train/Val/Test split (70/15/15)\n",
    "train, temp = train_test_split(\n",
    "    df, test_size=0.30, random_state=42, stratify=df['target']\n",
    ")\n",
    "validate, test = train_test_split(\n",
    "    temp, test_size=0.50, random_state=42, stratify=temp['target']\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train)}  Validate: {len(validate)}  Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 paths:\n",
      "  s3://c182567a4701745l12374482t1w262869721852-labbucket-jw0boyepjqpu/lab3/train/flight_delay_train.csv\n",
      "  s3://c182567a4701745l12374482t1w262869721852-labbucket-jw0boyepjqpu/lab3/validate/flight_delay_validate.csv\n",
      "  s3://c182567a4701745l12374482t1w262869721852-labbucket-jw0boyepjqpu/lab3/test/flight_delay_test.csv\n"
     ]
    }
   ],
   "source": [
    "prefix = 'lab3'                  # change if you like\n",
    "\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    # header=False, index=False like your format\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False)\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "# Filenames (mirroring your naming convention)\n",
    "train_file    = 'flight_delay_train.csv'\n",
    "validate_file = 'flight_delay_validate.csv'\n",
    "test_file     = 'flight_delay_test.csv'\n",
    "\n",
    "# Upload splits\n",
    "upload_s3_csv(train_file, 'train', train)\n",
    "upload_s3_csv(validate_file, 'validate', validate)\n",
    "upload_s3_csv(test_file, 'test', test)\n",
    "\n",
    "print(f\"S3 paths:\\n  s3://{bucket}/{prefix}/train/{train_file}\\n  s3://{bucket}/{prefix}/validate/{validate_file}\\n  s3://{bucket}/{prefix}/test/{test_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-30-12-53-43-736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-10-30 12:53:45 Starting - Starting the training job.\n",
      "2025-10-30 12:53:59 Starting - Preparing the instances for training....\n",
      "2025-10-30 12:54:23 Downloading - Downloading input data.....\n",
      "2025-10-30 12:54:53 Downloading - Downloading the training image..........\n",
      "2025-10-30 12:55:49 Training - Training image download completed. Training in progress....\n",
      "2025-10-30 12:56:10 Uploading - Uploading generated training model...\n",
      "2025-10-30 12:56:28 Completed - Training job completed\n",
      "ready for hosting!\n"
     ]
    }
   ],
   "source": [
    "container = image_uris.retrieve('xgboost',boto3.Session().region_name,'1.0-1')\n",
    "\n",
    "hyperparams={\"num_round\":\"42\",\n",
    "             \"eval_metric\": \"auc\",\n",
    "             \"objective\": \"binary:logistic\"}\n",
    "\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)\n",
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                       sagemaker.get_execution_role(),\n",
    "                                       instance_count=1,\n",
    "                                       instance_type='ml.m4.xlarge',\n",
    "                                       output_path=s3_output_location,\n",
    "                                        hyperparameters=hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())\n",
    "\n",
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}\n",
    "\n",
    "xgb_model.fit(inputs=data_channels, logs=False)\n",
    "\n",
    "print('ready for hosting!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-30-12-57-34-652\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2025-10-30-12-57-34-652\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2025-10-30-12-57-34-652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb_model.deploy(initial_instance_count=1,\n",
    "                serializer = sagemaker.serializers.CSVSerializer(),\n",
    "                instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: ['Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO', 'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5', 'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9', 'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12', 'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15', 'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18', 'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21', 'DepHourofDay_22', 'DepHourofDay_23']\n"
     ]
    }
   ],
   "source": [
    "non_numeric_cols = test.select_dtypes(exclude=['number']).columns\n",
    "print(\"Non-numeric columns:\", list(non_numeric_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean True/False to 1/0\n",
    "for col in test.select_dtypes(include=['bool']).columns:\n",
    "    test[col] = test[col].astype(int)\n",
    "\n",
    "# Convert 'True'/'False' strings to numeric 1/0\n",
    "for col in test.columns:\n",
    "    if test[col].dtype == object:\n",
    "        if set(test[col].unique()) <= {'True', 'False'}:\n",
    "            test[col] = test[col].map({'True': 1, 'False': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: []\n"
     ]
    }
   ],
   "source": [
    "non_numeric_cols = test.select_dtypes(exclude=['number']).columns\n",
    "print(\"Non-numeric columns:\", list(non_numeric_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Quarter_2</th>\n",
       "      <th>Quarter_3</th>\n",
       "      <th>Quarter_4</th>\n",
       "      <th>Month_2</th>\n",
       "      <th>Month_3</th>\n",
       "      <th>Month_4</th>\n",
       "      <th>Month_5</th>\n",
       "      <th>Month_6</th>\n",
       "      <th>...</th>\n",
       "      <th>DepHourofDay_14</th>\n",
       "      <th>DepHourofDay_15</th>\n",
       "      <th>DepHourofDay_16</th>\n",
       "      <th>DepHourofDay_17</th>\n",
       "      <th>DepHourofDay_18</th>\n",
       "      <th>DepHourofDay_19</th>\n",
       "      <th>DepHourofDay_20</th>\n",
       "      <th>DepHourofDay_21</th>\n",
       "      <th>DepHourofDay_22</th>\n",
       "      <th>DepHourofDay_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234564</th>\n",
       "      <td>0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32337</th>\n",
       "      <td>0</td>\n",
       "      <td>802.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10046</th>\n",
       "      <td>1</td>\n",
       "      <td>370.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228703</th>\n",
       "      <td>0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101846</th>\n",
       "      <td>0</td>\n",
       "      <td>1199.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target  Distance  Quarter_2  Quarter_3  Quarter_4  Month_2  Month_3  \\\n",
       "234564       0     602.0          0          0          0        0        1   \n",
       "32337        0     802.0          0          0          1        0        0   \n",
       "10046        1     370.0          1          0          0        0        0   \n",
       "228703       0     862.0          1          0          0        0        0   \n",
       "101846       0    1199.0          1          0          0        0        0   \n",
       "\n",
       "        Month_4  Month_5  Month_6  ...  DepHourofDay_14  DepHourofDay_15  \\\n",
       "234564        0        0        0  ...                0                0   \n",
       "32337         0        0        0  ...                1                0   \n",
       "10046         1        0        0  ...                0                0   \n",
       "228703        0        0        1  ...                0                0   \n",
       "101846        1        0        0  ...                0                0   \n",
       "\n",
       "        DepHourofDay_16  DepHourofDay_17  DepHourofDay_18  DepHourofDay_19  \\\n",
       "234564                0                0                0                0   \n",
       "32337                 0                0                0                0   \n",
       "10046                 0                0                0                0   \n",
       "228703                0                0                0                0   \n",
       "101846                0                0                0                0   \n",
       "\n",
       "        DepHourofDay_20  DepHourofDay_21  DepHourofDay_22  DepHourofDay_23  \n",
       "234564                0                0                0                0  \n",
       "32337                 0                0                0                0  \n",
       "10046                 0                0                0                0  \n",
       "228703                0                0                0                0  \n",
       "101846                0                1                0                0  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import BytesDeserializer\n",
    "import numpy as np\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test.drop('target', axis=1).astype('float32')\n",
    "y_test = test['target'].astype(int).to_numpy()\n",
    "\n",
    "# Configure predictor\n",
    "xgb_predictor.serializer   = CSVSerializer()\n",
    "xgb_predictor.deserializer = BytesDeserializer()  # return raw bytes\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.accept       = 'text/csv'\n",
    "\n",
    "def parse_scores(resp_bytes, expect_n=None):\n",
    "    \"\"\"Parse CSV text that may be one line with commas or many lines.\"\"\"\n",
    "    text = resp_bytes.decode('utf-8', errors='ignore').strip()\n",
    "    # Split on commas, whitespace, or newlines; filter empties\n",
    "    tokens = [t for t in re.split(r'[,\\r\\n\\s]+', text) if t]\n",
    "    scores = np.array([float(t) for t in tokens], dtype=float)\n",
    "    if expect_n is not None and scores.size != expect_n:\n",
    "        print(f\"Parsed {scores.size} scores, expected {expect_n}. First 120 chars:\\n{text[:120]}\")\n",
    "    return scores\n",
    "\n",
    "pred_scores = []\n",
    "batch_size = 200\n",
    "\n",
    "for start in range(0, len(X_test), batch_size):\n",
    "    batch = X_test.iloc[start:start + batch_size]\n",
    "    payload = batch.to_csv(header=False, index=False).strip()\n",
    "    resp = xgb_predictor.predict(payload)           # bytes\n",
    "    scores = parse_scores(resp, expect_n=len(batch))\n",
    "    pred_scores.extend(scores.tolist())\n",
    "\n",
    "pred_scores = np.array(pred_scores, dtype=float)\n",
    "pred_labels = (pred_scores >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7915749572022499\n",
      "Precision: 0.0\n",
      "Recall   : 0.0\n",
      "F1-score : 0.0\n",
      "ROC AUC  : 0.5584638278292663\n",
      "\n",
      "Confusion Matrix:\n",
      " [[38841     0]\n",
      " [10227     0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88     38841\n",
      "           1       0.00      0.00      0.00     10227\n",
      "\n",
      "    accuracy                           0.79     49068\n",
      "   macro avg       0.40      0.50      0.44     49068\n",
      "weighted avg       0.63      0.79      0.70     49068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(y_test, pred_labels))\n",
    "print(\"Precision:\", precision_score(y_test, pred_labels, zero_division=0))\n",
    "print(\"Recall   :\", recall_score(y_test, pred_labels, zero_division=0))\n",
    "print(\"F1-score :\", f1_score(y_test, pred_labels, zero_division=0))\n",
    "try:\n",
    "    print(\"ROC AUC  :\", roc_auc_score(y_test, pred_scores))\n",
    "except Exception as e:\n",
    "    print(\"ROC AUC  : n/a (\", e, \")\")\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, pred_labels))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Linear and XGBoost on Combined Data:\n",
    "\n",
    "When comparing the Linear Learner and XGBoost models trained on the combined dataset v1 (70/15/15 split), the results clearly show that increasing the model complexity from a simple linear approach to an ensemble method did not produce substantial improvement in predictive performance. Both models achieved an overall accuracy of around 79%, but this metric is deceptive in the context of a highly imbalanced dataset—where the number of non-delayed flights vastly exceeds the number of delayed ones. Accuracy alone does not reflect true predictive capability for minority classes, and in this case, both models predominantly classified almost every flight as “on time,” ignoring the delayed cases. This issue is evident in the confusion matrices, where the majority of predictions fell within the true negative category, and very few (if any) positive predictions were made.\n",
    "\n",
    "The Linear Learner, which serves as a baseline linear classification model, performed marginally better than XGBoost in identifying at least a handful of delayed flights. It achieved a precision of 0.46, indicating that some of its positive predictions were correct, but its recall was only 0.0012, meaning it captured less than 1% of actual delayed cases. This resulted in a very low F1-score (0.0023), showing a poor balance between precision and recall. The confusion matrix confirms that while the model correctly classified most non-delayed flights (38,827 out of 38,841), it failed to detect almost all delayed ones (only 12 out of 10,227). The model’s inability to generalize delay patterns suggests that the relationships between the input features and delay events are either weakly linear or overshadowed by the dominance of the majority class.\n",
    "\n",
    "The XGBoost model, a more sophisticated ensemble learner that typically excels in handling non-linear relationships and complex feature interactions, surprisingly performed no better. It yielded zero precision, recall, and F1-score for the delayed class, with a ROC AUC of 0.558, only marginally above random guessing. The confusion matrix reveals that XGBoost classified all flights as non-delayed, completely neglecting the minority class. This outcome suggests that the model overfitted to the dominant class during training or that its hyperparameters (such as scale_pos_weight, learning rate, or number of estimators) were not tuned to handle the severe class imbalance. Essentially, XGBoost failed to leverage its usual strengths in this setting because the imbalance in class distribution overwhelmed the model’s decision boundaries.\n",
    "\n",
    "Overall, both models demonstrate the limitations of using standard training pipelines on imbalanced datasets. The Linear Learner offered minimal detection of delayed flights, whereas XGBoost, despite being an advanced ensemble algorithm, defaulted entirely to the majority prediction. This indicates that model complexity alone cannot overcome data imbalance issues without appropriate preprocessing and tuning strategies. To achieve meaningful improvement, future iterations should incorporate techniques such as resampling (SMOTE or undersampling), adjusting class weights, or focusing on alternative metrics like Precision-Recall AUC. Additionally, introducing more discriminative features, such as weather severity indices, traffic congestion, or time-of-day patterns, may help capture non-linear dependencies that simple and ensemble models alike failed to recognize in this experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

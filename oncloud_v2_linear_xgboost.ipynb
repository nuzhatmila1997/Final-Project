{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Predicting Airplane Delays\n",
    "\n",
    "The goals of this notebook are:\n",
    "- Process and create a dataset from downloaded ZIP files\n",
    "- Exploratory data analysis (EDA)\n",
    "- Establish a baseline model and improve it\n",
    "\n",
    "## Introduction to business scenario\n",
    "You work for a travel booking website that is working to improve the customer experience for flights that were delayed. The company wants to create a feature to let customers know if the flight will be delayed due to weather when the customers are booking the flight to or from the busiest airports for domestic travel in the US. \n",
    "\n",
    "You are tasked with solving part of this problem by leveraging machine learning to identify whether the flight will be delayed due to weather. You have been given access to the a dataset of on-time performance of domestic flights operated by large air carriers. You can use this data to train a machine learning model to predict if the flight is going to be delayed for the busiest airports.\n",
    "\n",
    "### Dataset\n",
    "The provided dataset contains scheduled and actual departure and arrival times reported by certified US air carriers that account for at least 1 percent of domestic scheduled passenger revenues. The data was collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS). The dataset contains date, time, origin, destination, airline, distance, and delay status of flights for flights between 2014 and 2018.\n",
    "The data are in 60 compressed files, where each file contains a CSV for the flight details in a month for the five years (from 2014 - 2018). The data can be downloaded from this [link](https://ucstaff-my.sharepoint.com/:f:/g/personal/ibrahim_radwan_canberra_edu_au/EhWeqeQsh-9Mr1fneZc9_0sBOBzEdXngvxFJtAlIa-eAgA?e=8ukWwa). Please download the data files and place them on a relative path. Dataset(s) used in this assignment were compiled by the Office of Airline Information, Bureau of Transportation Statistics (BTS), Airline On-Time Performance Data, available with the following [link](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the environment \n",
    "\n",
    "Use one of the labs which we have practised on with the Amazon Sagemakers where you perform the following steps:\n",
    "1. Start a lab.\n",
    "2. Create a notebook instance and name it \"oncloudproject\".\n",
    "3. Increase the used memory to 25 GB from the additional configurations.\n",
    "4. Open Jupyter Lab and upload this notebook into it.\n",
    "5. Upload the two combined CVS files (combined_csv_v1.csv and combined_csv_v2.csv), which you created in Part A of this project.\n",
    "\n",
    "**Note:** In case of the data is too much to be uploaded to the AWS, please use 20% of the data only for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Build and evaluate simple models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use linear learner estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey and to comments on the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os, io, boto3, zipfile, requests\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='c182567a4701745l12374482t1w921201583050-labbucket-9dbcd7xdlpx6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 228982  Validate: 49068  Test: 49068\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('combined_csv_v2_20percent.csv')\n",
    "\n",
    "# label column is named 'target' and is first column \n",
    "if 'target' not in df.columns:\n",
    "    raise ValueError(\"Expected a 'target' column in the dataset.\")\n",
    "cols = df.columns.tolist()\n",
    "cols = ['target'] + [c for c in cols if c != 'target']\n",
    "df = df[cols]\n",
    "\n",
    "# Optional: ensure target is 0/1 ints\n",
    "df['target'] = pd.to_numeric(df['target'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Train/Val/Test split (70/15/15)\n",
    "train, temp = train_test_split(\n",
    "    df, test_size=0.30, random_state=42, stratify=df['target']\n",
    ")\n",
    "validate, test = train_test_split(\n",
    "    temp, test_size=0.50, random_state=42, stratify=temp['target']\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train)}  Validate: {len(validate)}  Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 paths:\n",
      "  s3://c182567a4701745l12374482t1w921201583050-labbucket-9dbcd7xdlpx6/lab3/train/flight_delay_train.csv\n",
      "  s3://c182567a4701745l12374482t1w921201583050-labbucket-9dbcd7xdlpx6/lab3/validate/flight_delay_validate.csv\n",
      "  s3://c182567a4701745l12374482t1w921201583050-labbucket-9dbcd7xdlpx6/lab3/test/flight_delay_test.csv\n"
     ]
    }
   ],
   "source": [
    "prefix = 'lab3'                  # change if you like\n",
    "\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    # header=False, index=False like your format\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False)\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "# Filenames (mirroring your naming convention)\n",
    "train_file    = 'flight_delay_train.csv'\n",
    "validate_file = 'flight_delay_validate.csv'\n",
    "test_file     = 'flight_delay_test.csv'\n",
    "\n",
    "# Upload splits\n",
    "upload_s3_csv(train_file, 'train', train)\n",
    "upload_s3_csv(validate_file, 'validate', validate)\n",
    "upload_s3_csv(test_file, 'test', test)\n",
    "\n",
    "print(f\"S3 paths:\\n  s3://{bucket}/{prefix}/train/{train_file}\\n  s3://{bucket}/{prefix}/validate/{validate_file}\\n  s3://{bucket}/{prefix}/test/{test_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = image_uris.retrieve('linear-learner', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)\n",
    "# Create estimator\n",
    "linear = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "# Set hyperparameters for classification\n",
    "linear.set_hyperparameters(\n",
    "    predictor_type='binary_classifier',\n",
    "    mini_batch_size=200,\n",
    "    epochs=10,\n",
    "    loss='logistic',\n",
    "    normalize_data=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: linear-learner-2025-10-30-09-48-46-671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-10-30 09:48:51 Starting - Starting the training job..\n",
      "2025-10-30 09:49:06 Starting - Preparing the instances for training....\n",
      "2025-10-30 09:49:31 Downloading - Downloading input data........\n",
      "2025-10-30 09:50:16 Downloading - Downloading the training image...............\n",
      "2025-10-30 09:51:37 Training - Training image download completed. Training in progress..................................................................................................\n",
      "2025-10-30 09:59:49 Uploading - Uploading generated training model.\n",
      "2025-10-30 10:00:02 Completed - Training job completed\n",
      "ready for hosting!\n"
     ]
    }
   ],
   "source": [
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}\n",
    "\n",
    "linear.fit(inputs=data_channels, logs=False)\n",
    "\n",
    "print('ready for hosting!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: linear-learner-2025-10-30-10-00-11-614\n",
      "INFO:sagemaker:Creating endpoint-config with name linear-learner-2025-10-30-10-00-11-614\n",
      "INFO:sagemaker:Creating endpoint with name linear-learner-2025-10-30-10-00-11-614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = linear.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json, numpy as np\n",
    "\n",
    "# Make sure features are numeric float32\n",
    "X_test = test.drop('target', axis=1).astype('float32')\n",
    "y_test = test['target']\n",
    "\n",
    "# Configure endpoint I/O\n",
    "predictor.serializer   = CSVSerializer()        # send CSV\n",
    "predictor.deserializer = JSONDeserializer()     # receive JSON\n",
    "predictor.content_type = 'text/csv'\n",
    "predictor.accept       = 'application/json'\n",
    "\n",
    "pred_labels, pred_scores = [], []\n",
    "batch_size = 200\n",
    "\n",
    "for start in range(0, len(X_test), batch_size):\n",
    "    batch = X_test.iloc[start:start+batch_size]\n",
    "    payload = batch.to_csv(header=False, index=False).strip()  # no trailing newline\n",
    "\n",
    "    resp = predictor.predict(payload)          # <-- now a dict\n",
    "    preds = resp['predictions']                # list of dicts\n",
    "\n",
    "    for p in preds:\n",
    "        pred_labels.append(int(p['predicted_label']))\n",
    "        pred_scores.append(float(p['score']))\n",
    "\n",
    "pred_labels = np.array(pred_labels)\n",
    "pred_scores = np.array(pred_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy    : 0.7943\n",
      "Precision   : 0.5702\n",
      "Recall      : 0.0524\n",
      "F1-score    : 0.0960\n",
      "Confusion Matrix:\n",
      " [[38437   404]\n",
      " [ 9691   536]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88     38841\n",
      "           1       0.57      0.05      0.10     10227\n",
      "\n",
      "    accuracy                           0.79     49068\n",
      "   macro avg       0.68      0.52      0.49     49068\n",
      "weighted avg       0.75      0.79      0.72     49068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
    "acc  = accuracy_score(y_test, pred_labels)\n",
    "prec = precision_score(y_test, pred_labels, zero_division=0)\n",
    "rec  = recall_score(y_test, pred_labels, zero_division=0)\n",
    "f1   = f1_score(y_test, pred_labels, zero_division=0)\n",
    "cm   = confusion_matrix(y_test, pred_labels)\n",
    "\n",
    "print(f\"Accuracy    : {acc:.4f}\")\n",
    "print(f\"Precision   : {prec:.4f}\")\n",
    "print(f\"Recall      : {rec:.4f}\")\n",
    "print(f\"F1-score    : {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Build and evaluate ensembe models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use xgboost estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "6. write down your observation on the difference between the performance of using the simple and ensemble models.\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-30-10-15-19-039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-10-30 10:15:20 Starting - Starting the training job..\n",
      "2025-10-30 10:15:35 Starting - Preparing the instances for training...\n",
      "2025-10-30 10:15:59 Downloading - Downloading input data......\n",
      "2025-10-30 10:16:30 Downloading - Downloading the training image.........\n",
      "2025-10-30 10:17:20 Training - Training image download completed. Training in progress........\n",
      "2025-10-30 10:18:01 Uploading - Uploading generated training model.\n",
      "2025-10-30 10:18:14 Completed - Training job completed\n",
      "ready for hosting!\n"
     ]
    }
   ],
   "source": [
    "# train test val previously calculated for v2\n",
    "container = image_uris.retrieve('xgboost',boto3.Session().region_name,'1.0-1')\n",
    "\n",
    "hyperparams={\"num_round\":\"42\",\n",
    "             \"eval_metric\": \"auc\",\n",
    "             \"objective\": \"binary:logistic\"}\n",
    "\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)\n",
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                       sagemaker.get_execution_role(),\n",
    "                                       instance_count=1,\n",
    "                                       instance_type='ml.m4.xlarge',\n",
    "                                       output_path=s3_output_location,\n",
    "                                        hyperparameters=hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())\n",
    "\n",
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}\n",
    "\n",
    "xgb_model.fit(inputs=data_channels, logs=False)\n",
    "\n",
    "print('ready for hosting!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-30-10-19-41-170\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2025-10-30-10-19-41-170\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2025-10-30-10-19-41-170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb_model.deploy(initial_instance_count=1,\n",
    "                serializer = sagemaker.serializers.CSVSerializer(),\n",
    "                instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>DepHourofDay</th>\n",
       "      <th>AWND_O</th>\n",
       "      <th>PRCP_O</th>\n",
       "      <th>TAVG_O</th>\n",
       "      <th>AWND_D</th>\n",
       "      <th>PRCP_D</th>\n",
       "      <th>TAVG_D</th>\n",
       "      <th>SNOW_O</th>\n",
       "      <th>SNOW_D</th>\n",
       "      <th>...</th>\n",
       "      <th>Origin_SFO</th>\n",
       "      <th>Dest_CLT</th>\n",
       "      <th>Dest_DEN</th>\n",
       "      <th>Dest_DFW</th>\n",
       "      <th>Dest_IAH</th>\n",
       "      <th>Dest_LAX</th>\n",
       "      <th>Dest_ORD</th>\n",
       "      <th>Dest_PHX</th>\n",
       "      <th>Dest_SFO</th>\n",
       "      <th>is_holiday_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234564</th>\n",
       "      <td>602.0</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32337</th>\n",
       "      <td>802.0</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10046</th>\n",
       "      <td>370.0</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228703</th>\n",
       "      <td>862.0</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101846</th>\n",
       "      <td>1199.0</td>\n",
       "      <td>21</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Distance  DepHourofDay  AWND_O  PRCP_O  TAVG_O  AWND_D  PRCP_D  \\\n",
       "234564     602.0            12      43       0   -69.0      18       0   \n",
       "32337      802.0            14      33       0   115.0      51       0   \n",
       "10046      370.0            12      34       0   186.0      40       0   \n",
       "228703     862.0             9      36       0   194.0      50       0   \n",
       "101846    1199.0            21      42       0   171.0      64       0   \n",
       "\n",
       "        TAVG_D  SNOW_O  SNOW_D  ...  Origin_SFO  Dest_CLT  Dest_DEN  Dest_DFW  \\\n",
       "234564   157.0     0.0     0.0  ...       False     False     False     False   \n",
       "32337    214.0     0.0     0.0  ...       False     False     False      True   \n",
       "10046    197.0     0.0     0.0  ...       False     False     False     False   \n",
       "228703   284.0     0.0     0.0  ...       False     False      True     False   \n",
       "101846   192.0     0.0     0.0  ...       False     False      True     False   \n",
       "\n",
       "        Dest_IAH  Dest_LAX  Dest_ORD  Dest_PHX  Dest_SFO  is_holiday_True  \n",
       "234564     False     False     False      True     False            False  \n",
       "32337      False     False     False     False     False            False  \n",
       "10046      False     False     False      True     False            False  \n",
       "228703     False     False     False     False     False            False  \n",
       "101846     False     False     False     False     False            False  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_X = test.iloc[:,1:];\n",
    "batch_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X_file='batch-in.csv'\n",
    "upload_s3_csv(batch_X_file, 'batch-in', batch_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import BytesDeserializer\n",
    "import numpy as np\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test.drop('target', axis=1).astype('float32')\n",
    "y_test = test['target'].astype(int).to_numpy()\n",
    "\n",
    "# Configure predictor\n",
    "xgb_predictor.serializer   = CSVSerializer()\n",
    "xgb_predictor.deserializer = BytesDeserializer()  # return raw bytes\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.accept       = 'text/csv'\n",
    "\n",
    "def parse_scores(resp_bytes, expect_n=None):\n",
    "    \"\"\"Parse CSV text that may be one line with commas or many lines.\"\"\"\n",
    "    text = resp_bytes.decode('utf-8', errors='ignore').strip()\n",
    "    # Split on commas, whitespace, or newlines; filter empties\n",
    "    tokens = [t for t in re.split(r'[,\\r\\n\\s]+', text) if t]\n",
    "    scores = np.array([float(t) for t in tokens], dtype=float)\n",
    "    if expect_n is not None and scores.size != expect_n:\n",
    "        print(f\"Parsed {scores.size} scores, expected {expect_n}. First 120 chars:\\n{text[:120]}\")\n",
    "    return scores\n",
    "\n",
    "pred_scores = []\n",
    "batch_size = 200\n",
    "\n",
    "for start in range(0, len(X_test), batch_size):\n",
    "    batch = X_test.iloc[start:start + batch_size]\n",
    "    payload = batch.to_csv(header=False, index=False).strip()\n",
    "    resp = xgb_predictor.predict(payload)           # bytes\n",
    "    scores = parse_scores(resp, expect_n=len(batch))\n",
    "    pred_scores.extend(scores.tolist())\n",
    "\n",
    "pred_scores = np.array(pred_scores, dtype=float)\n",
    "pred_labels = (pred_scores >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8012757805494416\n",
      "Precision: 0.64\n",
      "Recall   : 0.10638505915713307\n",
      "F1-score : 0.18244319610966714\n",
      "ROC AUC  : 0.693120653833251\n",
      "\n",
      "Confusion Matrix:\n",
      " [[38229   612]\n",
      " [ 9139  1088]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.89     38841\n",
      "           1       0.64      0.11      0.18     10227\n",
      "\n",
      "    accuracy                           0.80     49068\n",
      "   macro avg       0.72      0.55      0.53     49068\n",
      "weighted avg       0.77      0.80      0.74     49068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(y_test, pred_labels))\n",
    "print(\"Precision:\", precision_score(y_test, pred_labels, zero_division=0))\n",
    "print(\"Recall   :\", recall_score(y_test, pred_labels, zero_division=0))\n",
    "print(\"F1-score :\", f1_score(y_test, pred_labels, zero_division=0))\n",
    "try:\n",
    "    print(\"ROC AUC  :\", roc_auc_score(y_test, pred_scores))\n",
    "except Exception as e:\n",
    "    print(\"ROC AUC  : n/a (\", e, \")\")\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, pred_labels))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Linear and XGBoost on Combined Data:\n",
    "\n",
    "The results from the combined dataset v2 show a clear improvement in overall performance for both the simple Linear Learner model and the ensemble XGBoost model compared to the earlier version (v1). However, the XGBoost model demonstrates a stronger ability to capture delayed flights, indicating that ensemble learning provides better predictive power and sensitivity to complex patterns in the data.\n",
    "\n",
    "Starting with the Linear Learner, it achieved an accuracy of 79.43%, a precision of 0.57, and a recall of 0.0524, resulting in an F1-score of 0.0960. These metrics indicate that while the model was able to correctly classify most non-delayed flights (majority class), it struggled significantly in identifying the minority class — delayed flights. The confusion matrix confirms this imbalance: out of 10,227 delayed flights, only 536 were correctly identified, while 9,691 were missed. Although the linear model has learned some separation between classes, its linear decision boundary likely limits its ability to capture non-linear interactions between features such as weather, time, and route characteristics. This led to high precision (when it predicted a delay, it was often correct) but very low recall (it failed to detect most delays).\n",
    "\n",
    "The XGBoost model, on the other hand, achieved a slightly higher accuracy of 80.12%, and showed substantial improvements in all minority-class metrics: precision increased to 0.64, recall to 0.106, and F1-score to 0.182. The ROC-AUC score of 0.693 also suggests that XGBoost learned a more discriminative decision boundary compared to the near-random performance seen in the previous dataset version. The confusion matrix shows that XGBoost correctly identified 1,088 delayed flights, roughly double that of the Linear Learner, while maintaining strong performance on non-delayed flights (38,229 true negatives). This demonstrates XGBoost’s ability to handle feature interactions and non-linear patterns, leveraging its boosting mechanism to sequentially minimize misclassifications that a simple linear model cannot adapt to.\n",
    "\n",
    "In summary, while both models still suffer from the class imbalance problem, the XGBoost model performed notably better, particularly in identifying delayed flights. Its improvement in recall and F1-score highlights the advantage of ensemble methods in capturing complex relationships between features. The Linear Learner remains useful as a fast, interpretable baseline, but it lacks the flexibility to generalize effectively in data with high dimensionality and non-linear dependencies. To further improve both models, future work should focus on techniques such as class-weight adjustment, oversampling the delayed class, or threshold tuning, which would help enhance recall without overly compromising precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
